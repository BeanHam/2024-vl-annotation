{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aac8f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/binhan/anaconda3/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import string\n",
    "import base64\n",
    "import requests\n",
    "import argparse\n",
    "import numpy as np\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1cc881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_line_prompt = \"\"\"\n",
    "[INST] <image>\\n\n",
    "A stop line is a white line painted on the road at intersections where traffic must stop. \n",
    "It shows drivers where to halt their vehicles. \n",
    "Which labeled images represent stop line?[/INST]\n",
    "\"\"\"\n",
    "raised_table_prompt=\"\"\"\n",
    "A raised table usually covers the entire width of the crosswalk. \n",
    "It is typically painted with triangular arrows in white color.\n",
    "Which labeled images represent raised table?[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "af3d6c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_cal(gt, pred):\n",
    "    \"\"\"\n",
    "    Calculate IoU metric.\n",
    "    \"\"\"    \n",
    "    return np.logical_and(gt, pred).sum()/np.logical_or(gt, pred).sum() \n",
    "\n",
    "def mask_filtering(img, masks, obj, colors_to_remove, colors_to_stay):\n",
    "    \"\"\"\n",
    "    Filter segmented masks.\n",
    "    \"\"\"\n",
    "    filtered_masks = []\n",
    "    # filter based on area\n",
    "    if obj=='raised_table':\n",
    "        masks = [m for m in masks if m['area']>400]\n",
    "\n",
    "    # filter based on color\n",
    "    for mask in masks:\n",
    "        masked_color = img[mask['segmentation']].mean(axis=0).reshape(1,-1)\n",
    "        remove_dist = pairwise_distances(colors_to_remove, masked_color).min()\n",
    "        stay_dist = pairwise_distances(colors_to_stay, masked_color).min()\n",
    "        if remove_dist>stay_dist:\n",
    "            filtered_masks.append(mask)\n",
    "            \n",
    "    return filtered_masks\n",
    "\n",
    "def mask_visualization(img, masks, output_path,save_name):\n",
    "    \"\"\"\n",
    "    Generate visualization containing segments.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## visualize potential candidates\n",
    "    col = 5\n",
    "    row = len(masks)//col+1*(len(masks)%col>0)\n",
    "    fig, axs = plt.subplots(row, col, figsize=(col*3, row*3))\n",
    "    plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "    for r in range(row):\n",
    "        for c in range(col):\n",
    "\n",
    "            if row == 1:\n",
    "                ## outside of range\n",
    "                if r*5+c >=len(masks):\n",
    "                    axs[r*5+c].axis('off')\n",
    "                    continue\n",
    "\n",
    "                ## extract bounding box\n",
    "                bbox = masks[r*5+c]['bbox']\n",
    "                xtl, ytl = int(bbox[0]), int(bbox[1])\n",
    "                xbr, ybr = int(xtl+bbox[2]), int(ytl+bbox[3])\n",
    "                axs[c].imshow(img[ytl:ybr, xtl:xbr])\n",
    "                axs[c].set_title(f'{r*5+c}')\n",
    "                axs[c].axis('off')\n",
    "\n",
    "            else:\n",
    "                ## outside of range\n",
    "                if r*5+c >=len(masks):\n",
    "                    axs[r,c].axis('off')\n",
    "                    continue\n",
    "\n",
    "                ## extract bounding box\n",
    "                bbox = masks[r*5+c]['bbox']\n",
    "                xtl, ytl = int(bbox[0]), int(bbox[1])\n",
    "                xbr, ybr = int(xtl+bbox[2]), int(ytl+bbox[3])\n",
    "                axs[r,c].imshow(img[ytl:ybr, xtl:xbr])\n",
    "                axs[r,c].set_title(f'{r*5+c}')\n",
    "                axs[r,c].axis('off')\n",
    "    plt.savefig(output_path+save_name+'_candidates.png', bbox_inches='tight', dpi=600, pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def write_completion_request(prompt, base64_image):\n",
    "    \"\"\"\n",
    "    Compose completion request.\n",
    "    \"\"\"\n",
    "    \n",
    "    completion = {\n",
    "      \"model\": \"gpt-4-turbo-2024-04-09\",\n",
    "      \"messages\": [\n",
    "          {\"role\": \"user\",\n",
    "           \"content\": [\n",
    "               {\"type\": \"text\", \"text\": prompt},\n",
    "               {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "           ]}\n",
    "      ],\n",
    "      \"max_tokens\": 200\n",
    "    }\n",
    "    return completion\n",
    "\n",
    "def post_processing(response, masks, output_path, save_name):\n",
    "    \"\"\"\n",
    "    Extract bounding box.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = response.translate(str.maketrans('', '', string.punctuation))\n",
    "        labels = [int(l) for l in response.split() if l.isnumeric()]        \n",
    "        bboxes = [masks[l]['bbox'] for l in labels]\n",
    "        masking = [masks[l]['segmentation'] for l in labels]        \n",
    "        x1 = masking[0]\n",
    "        for i in range(1, len(masking)):\n",
    "            x2 = masking[i]\n",
    "            x1 = np.logical_or(x1, x2)\n",
    "        masking=x1\n",
    "    except:\n",
    "        labels = [-1]\n",
    "        bboxes = [[0,0,0,0]]\n",
    "        masking = np.zeros(masks[0]['segmentation'].shape)    \n",
    "\n",
    "    np.save(output_path+save_name+'_bbox.npy', bboxes)\n",
    "    np.save(output_path+save_name+'_masking.npy', masking)\n",
    "    \n",
    "    return masking\n",
    "\n",
    "def final_visualization(img, masking, output_path,save_name):\n",
    "    plt.figure(figsize=(3.36,3.36))\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)    \n",
    "    img_mask = np.ones((masking.shape[0], masking.shape[1], 4))\n",
    "    img_mask[:,:,3] = 0\n",
    "    img_mask[masking] = np.concatenate([[1,0,0], [0.5]])\n",
    "    ax.imshow(img_mask)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(output_path+save_name+'_masking.png', bbox_inches='tight', dpi=600, pad_inches=0)\n",
    "    plt.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "edbf7448",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = 'raised_table'\n",
    "api_key = 'sk-proj-iqZpYoNdT94oQAwnSoQtT3BlbkFJdrklgzQFuYRGcJnEXiia'\n",
    "method='vg-no-context'\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "sam_model_type = \"vit_h\"\n",
    "image_size = (336,336)\n",
    "image_path = f'images/{obj}/'\n",
    "image_names = os.listdir(image_path)\n",
    "image_names = [name for name in image_names if ((name.endswith('.png') & ('masking' not in name)))]\n",
    "output_path = f'outputs/{method}/{obj}/'\n",
    "api_web = \"https://api.openai.com/v1/chat/completions\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if not os.path.isdir(output_path):\n",
    "    os.makedirs(output_path)\n",
    "if obj == \"stop_line\":\n",
    "    prompt = stop_line_prompt\n",
    "else:\n",
    "    prompt = raised_table_prompt        \n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "colors_to_remove=np.array([\n",
    "    # green colors\n",
    "    [124,252,0],[127,255,0],[50,205,50],[0,255,0],[34,139,34],[0,128,0],[0,100,0],\n",
    "    [173,255,47],[154,205,50],[0,255,127],[0,250,154],[144,238,144],[152,251,152],\n",
    "    [143,188,143],[60,179,113],[32,178,170],[46,139,87],[128,128,0],[85,107,47],[107,142,35],        \n",
    "    # yellow colors\n",
    "    [255,228,181],[255,218,185],[238,232,170],[240,230,140],[189,183,107],[255,255,0],\n",
    "    [128,128,0],[173,255,47],[154,205,50],[255,255,153],[255,255,102],[255,255,51],[255,255,0],\n",
    "    [204,204,0],[153,153,0],[102,102,0],[51,51,0],        \n",
    "    # brown colors\n",
    "    [222,184,135],[210,180,140],[188,143,143],[244,164,96],[218,165,32],[205,133,63],\n",
    "    [210,105,30],[139,69,19],[160,82,45],[165,42,42],[128,0,0],\n",
    "    \n",
    "])\n",
    "if obj=='stop_line':\n",
    "    colors_to_stay=np.array([\n",
    "        # white colors\n",
    "        [255,255,255],[255,250,250],[245,255,250],[240,255,255],[248,248,255],[245,245,245],        \n",
    "        # silver colors\n",
    "        [220,220,220],[211,211,211],[192,192,192],[169,169,169]\n",
    "    ])\n",
    "else:\n",
    "    colors_to_stay=np.array([\n",
    "        # white colors\n",
    "        [255,255,255],[255,250,250],[245,255,250],[240,255,255],[248,248,255],[245,245,245],        \n",
    "        # silver colors\n",
    "        [220,220,220],[211,211,211],[192,192,192],[169,169,169],[128,128,128]#,[105,105,105]\n",
    "    ])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e61f6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = sam_model_registry[sam_model_type](checkpoint='../2024-vl-annotation-exp/sam_vit_h_4b8939.pth')\n",
    "sam.to(device)\n",
    "mask_generator = SamAutomaticMaskGenerator(model=sam, points_per_side=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "80f8a707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raised_table_10.png\n"
     ]
    }
   ],
   "source": [
    "image_name = image_names[2]\n",
    "print(image_name)\n",
    "save_name = image_name.split('.')[0]\n",
    "img = np.array(Image.open(image_path+image_name).convert('RGB').resize((336,336)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "b3a2b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = mask_generator.generate(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "0aa06b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "c8ba8603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_masks = mask_filtering(img, masks, obj, colors_to_remove, colors_to_stay)\n",
    "len(filtered_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "4bb9341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_visualization(img, filtered_masks, output_path, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "a69e9be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base64_image = encode_image(output_path+save_name+'_candidates.png')\n",
    "completion = write_completion_request(prompt, base64_image)\n",
    "response = requests.post(api_web, headers=headers, json=completion)\n",
    "response = response.json()['choices'][0]['message']['content']        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "9deb819f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Images that represent a raised table with white triangular arrows painted on them are image 7 and image 8. These images clearly show the characteristic white triangular markings typical for raised tables used in crosswalks.'"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "635992b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking = post_processing(response, filtered_masks, output_path, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "945f2560",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_visualization(img, masking, output_path, save_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
