{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba2df19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/binhan/anaconda3/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import string\n",
    "import base64\n",
    "import requests\n",
    "import argparse\n",
    "import numpy as np\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "\n",
    "stop_line_prompt = \"\"\"\n",
    "[INST] <image>\\n\n",
    "A stop line is a single white line painted on the road at intersections where traffic must stop.\n",
    "It shows drivers where to halt their vehicles. \n",
    "Which labeled images represent stop line?[/INST]\n",
    "\"\"\"\n",
    "raised_table_prompt=\"\"\"\n",
    "A raised table usually covers the entire width of the crosswalk. \n",
    "It is typically painted with triangular arrows in white color.\n",
    "Which labeled images represent raised table?[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3216c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_cal(gt, pred):\n",
    "    \"\"\"\n",
    "    Calculate IoU metric.\n",
    "    \"\"\"    \n",
    "    return np.logical_and(gt, pred).sum()/np.logical_or(gt, pred).sum() \n",
    "\n",
    "def mask_filtering(img, masks, obj, colors_to_remove, colors_to_stay):\n",
    "    \"\"\"\n",
    "    Filter segmented masks.\n",
    "    \"\"\"\n",
    "    filtered_masks = []\n",
    "    # filter based on area\n",
    "    if obj=='raised_table':\n",
    "        masks = [m for m in masks if m['area']>400]\n",
    "    else:\n",
    "        masks = [m for m in masks if m['area']>200]\n",
    "        \n",
    "    # filter based on color\n",
    "    for mask in masks:\n",
    "        masked_color = img[mask['segmentation']].mean(axis=0).reshape(1,-1)\n",
    "        remove_dist = pairwise_distances(colors_to_remove, masked_color).min()\n",
    "        stay_dist = pairwise_distances(colors_to_stay, masked_color).min()\n",
    "        if remove_dist>stay_dist:\n",
    "            filtered_masks.append(mask)\n",
    "            \n",
    "    return filtered_masks\n",
    "\n",
    "def no_context_mask_visualization(img, masks, output_path,save_name):\n",
    "    \"\"\"\n",
    "    Generate visualization containing segments.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## visualize potential candidates\n",
    "    col = 5\n",
    "    row = len(masks)//col+1*(len(masks)%col>0)\n",
    "    fig, axs = plt.subplots(row, col, figsize=(col*3, row*3))\n",
    "    plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "    for r in range(row):\n",
    "        for c in range(col):\n",
    "\n",
    "            if row == 1:\n",
    "                ## outside of range\n",
    "                if r*5+c >=len(masks):\n",
    "                    axs[r*5+c].axis('off')\n",
    "                    continue\n",
    "\n",
    "                ## extract bounding box\n",
    "                bbox = masks[r*5+c]['bbox']\n",
    "                xtl, ytl = int(bbox[0]), int(bbox[1])\n",
    "                xbr, ybr = int(xtl+bbox[2]), int(ytl+bbox[3])\n",
    "                axs[c].imshow(img[ytl:ybr, xtl:xbr])\n",
    "                axs[c].set_title(f'{r*5+c}')\n",
    "                axs[c].axis('off')\n",
    "\n",
    "            else:\n",
    "                ## outside of range\n",
    "                if r*5+c >=len(masks):\n",
    "                    axs[r,c].axis('off')\n",
    "                    continue\n",
    "\n",
    "                ## extract bounding box\n",
    "                bbox = masks[r*5+c]['bbox']\n",
    "                xtl, ytl = int(bbox[0]), int(bbox[1])\n",
    "                xbr, ybr = int(xtl+bbox[2]), int(ytl+bbox[3])\n",
    "                axs[r,c].imshow(img[ytl:ybr, xtl:xbr])\n",
    "                axs[r,c].set_title(f'{r*5+c}')\n",
    "                axs[r,c].axis('off')\n",
    "    plt.savefig(output_path+save_name+'_candidates_no_context.png', bbox_inches='tight', dpi=600, pad_inches=0)\n",
    "    plt.close()\n",
    "    \n",
    "def in_context_mask_visualization(img, masks, output_path,save_name, add_bbox):\n",
    "    \n",
    "    ## base image\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    img_mask = np.ones((masks[0]['segmentation'].shape[0], masks[0]['segmentation'].shape[1], 4))\n",
    "    img_mask[:,:,3] = 0\n",
    "    \n",
    "    ## visualize\n",
    "    counter = 0\n",
    "    for i in range(len(masks)):\n",
    "        ann = masks[i]\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
    "        img_mask[m] = color_mask\n",
    "        bboxes = [ann['bbox']]        \n",
    "        for bbox in bboxes:\n",
    "            xtl, ytl = int(bbox[0]), int(bbox[1])\n",
    "            xbr, ybr = int(xtl+bbox[2]), int(ytl+bbox[3])\n",
    "            rect = Rectangle((0.5*(xtl+xbr), 0.5*(ytl+ybr)-7), 8, 8, linewidth=0.1, facecolor='black')\n",
    "            ax.text(0.5*(xtl+xbr), 0.5*(ytl+ybr), str(counter), color='white', size=8)\n",
    "            ax.add_patch(rect)\n",
    "            ax.axis('off')\n",
    "            counter += 1\n",
    "            if add_bbox:\n",
    "                ax.hlines(ytl, xmin=xtl, xmax=xbr, color='red', linewidth=0.5)\n",
    "                ax.hlines(ybr, xmin=xtl, xmax=xbr, color='red', linewidth=0.5)\n",
    "                ax.vlines(xtl, ymin=ytl, ymax=ybr, color='red', linewidth=0.5)\n",
    "                ax.vlines(xbr, ymin=ytl, ymax=ybr, color='red', linewidth=0.5)              \n",
    "    ax.imshow(img_mask)   \n",
    "    plt.savefig(output_path+save_name+f'_candidates_in_context.png', bbox_inches='tight', dpi=600, pad_inches=0)\n",
    "    plt.close()\n",
    "    \n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def write_completion_request(prompt, no_context_image, in_context_image):\n",
    "    \"\"\"\n",
    "    Compose completion request.\n",
    "    \"\"\"\n",
    "    \n",
    "    completion = {\n",
    "      \"model\": \"gpt-4-turbo-2024-04-09\",\n",
    "      \"messages\": [\n",
    "          {\"role\": \"user\",\n",
    "           \"content\": [\n",
    "               {\"type\": \"text\", \"text\": prompt},\n",
    "               {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{no_context_image}\"}},\n",
    "               {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{in_context_image}\"}}\n",
    "           ]}\n",
    "      ],\n",
    "      \"max_tokens\": 200\n",
    "    }\n",
    "    return completion\n",
    "\n",
    "def post_processing(response, masks, output_path, save_name):\n",
    "    \"\"\"\n",
    "    Extract bounding box.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = response.translate(str.maketrans('', '', string.punctuation))\n",
    "        labels = [int(l) for l in response.split() if l.isnumeric()]        \n",
    "        bboxes = [masks[l]['bbox'] for l in labels]\n",
    "        masking = [masks[l]['segmentation'] for l in labels]        \n",
    "        x1 = masking[0]\n",
    "        for i in range(1, len(masking)):\n",
    "            x2 = masking[i]\n",
    "            x1 = np.logical_or(x1, x2)\n",
    "        masking=x1\n",
    "    except:\n",
    "        labels = [-1]\n",
    "        bboxes = [[0,0,0,0]]\n",
    "        masking = np.zeros(masks[0]['segmentation'].shape)    \n",
    "\n",
    "    np.save(output_path+save_name+'_bbox.npy', bboxes)\n",
    "    np.save(output_path+save_name+'_masking.npy', masking)\n",
    "    \n",
    "    return masking\n",
    "\n",
    "def final_visualization(img, masking, output_path,save_name):\n",
    "    plt.figure(figsize=(3.36,3.36))\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)    \n",
    "    img_mask = np.ones((masking.shape[0], masking.shape[1], 4))\n",
    "    img_mask[:,:,3] = 0\n",
    "    img_mask[masking==1] = np.concatenate([[1,0,0], [0.5]])\n",
    "    ax.imshow(img_mask)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(output_path+save_name+'_masking.png', bbox_inches='tight', dpi=600, pad_inches=0)\n",
    "    plt.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "038f616a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Load Parameters...\n",
      "-- Load SAM Model...\n"
     ]
    }
   ],
   "source": [
    "#-------------------------\n",
    "# arguments\n",
    "#-------------------------\n",
    "print('-- Load Parameters...')    \n",
    "obj = 'stop_line'\n",
    "api_key = 'sk-proj-iqZpYoNdT94oQAwnSoQtT3BlbkFJdrklgzQFuYRGcJnEXiia'\n",
    "method='vg-comb'\n",
    "add_bbox = False\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "sam_model_type = \"vit_h\"\n",
    "image_size = (336,336)\n",
    "image_path = f'images/{obj}/'\n",
    "image_names = os.listdir(image_path)\n",
    "image_names = [name for name in image_names if ((name.endswith('.png') & ('masking' not in name)))]\n",
    "output_path = f'outputs/{method}/bbox_{True}/{obj}/'\n",
    "api_web = \"https://api.openai.com/v1/chat/completions\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if not os.path.isdir(output_path):\n",
    "    os.makedirs(output_path)\n",
    "if obj == \"stop_line\":\n",
    "    prompt = stop_line_prompt\n",
    "else:\n",
    "    prompt = raised_table_prompt        \n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "    \n",
    "colors_to_remove=np.array([\n",
    "    # green colors\n",
    "    [124,252,0],[127,255,0],[50,205,50],[0,255,0],[34,139,34],[0,128,0],[0,100,0],\n",
    "    [173,255,47],[154,205,50],[0,255,127],[0,250,154],[144,238,144],[152,251,152],\n",
    "    [143,188,143],[60,179,113],[32,178,170],[46,139,87],[128,128,0],[85,107,47],[107,142,35],        \n",
    "    # yellow colors\n",
    "    [255,228,181],[255,218,185],[238,232,170],[240,230,140],[189,183,107],[255,255,0],\n",
    "    [128,128,0],[173,255,47],[154,205,50],[255,255,153],[255,255,102],[255,255,51],[255,255,0],\n",
    "    [204,204,0],[153,153,0],[102,102,0],[51,51,0],        \n",
    "    # brown colors\n",
    "    [222,184,135],[210,180,140],[188,143,143],[244,164,96],[218,165,32],[205,133,63],\n",
    "    [210,105,30],[139,69,19],[160,82,45],[165,42,42],[128,0,0],\n",
    "    \n",
    "])\n",
    "if obj=='stop_line':\n",
    "    colors_to_stay=np.array([\n",
    "        # white colors\n",
    "        [255,255,255],[255,250,250],[245,255,250],[240,255,255],[248,248,255],[245,245,245],        \n",
    "        # silver colors\n",
    "        [220,220,220],[211,211,211],[192,192,192],[169,169,169]\n",
    "    ])\n",
    "else:\n",
    "    colors_to_stay=np.array([\n",
    "        # white colors\n",
    "        [255,255,255],[255,250,250],[245,255,250],[240,255,255],[248,248,255],[245,245,245],        \n",
    "        # silver colors\n",
    "        [220,220,220],[211,211,211],[192,192,192],[169,169,169],[128,128,128]\n",
    "    ])        \n",
    "\n",
    "print('-- Load SAM Model...')    \n",
    "sam = sam_model_registry[sam_model_type](checkpoint='../2024-vl-annotation-exp/sam_vit_h_4b8939.pth')\n",
    "sam.to(device)\n",
    "mask_generator = SamAutomaticMaskGenerator(model=sam, points_per_side=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40b84a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = 'stop_line_1.png'\n",
    "save_name = image_name.split('.')[0]\n",
    "img = np.array(Image.open(image_path+image_name).convert('RGB').resize((336,336)))\n",
    "masks = mask_generator.generate(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f0064d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_masks = mask_filtering(img, masks, obj, colors_to_remove, colors_to_stay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99df1153",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_context_mask_visualization(img, filtered_masks, output_path, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b0a106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_context_mask_visualization(img, filtered_masks, output_path, save_name, add_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fca3735",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_context_image = encode_image(output_path+save_name+'_candidates_no_context.png')\n",
    "in_context_image = encode_image(output_path+save_name+'_candidates_in_context.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7758da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = write_completion_request(prompt, no_context_image, in_context_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb90fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(api_web, headers=headers, json=completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4fd3a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = response.json()['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ef6a109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the description of a stop line and the visual content provided in the images, the labeled images that represent stop lines are images 1 and 2. These images show horizontal white lines at intersections, similar to those typically used to indicate where vehicles must stop.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad8782ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking = post_processing(response, filtered_masks, output_path, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4fa28221",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_visualization(img, masking, output_path, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8f2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
